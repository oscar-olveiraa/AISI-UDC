# GEI AISI (UDC)

Aprender las herramientas:

* Apache Hadoop junto con su gestor de recursos planificador (YARN) y su sistema de ficheros distribuido (HDFS) en un clúster virtual basado en Debian GNU/Linux usando Vagrant, Ansible y NFS.

* Apache Spark, más eficiente y mas moderno que Apache Hadoop dado que usa memoria RAM para almacenar datos, mientras que Hadoop usa MapReduce guarda resultados intermedios en el disco duro, lo que ralentiza el proceso pero es eficiente para almacenamiento masivo a bajo coste.
